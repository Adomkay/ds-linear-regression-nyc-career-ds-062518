{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here's some data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we previously fit a regression model y=mx+b to predict domestic gross sales from a movies budget.  \n",
    "Expanding this example, we can imagine being in charge of managing a large production and we might want to figure out what other factors such as the actors, director, genre, running length or other features were most predictive of the movie's success in the box office.\n",
    "\n",
    "Here gradient descent is slightly more complicated as we're dealing with the multivariate case. As a result, we can take the derivative (or gradient) with respect to different variables. The underlying intuition is that we want to move in the direction of the steepest descent in hopes that we can find the global minimum. We then do this through a series of successive steepest steps forward until we are satisfied with the result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the data. It's stored in a file called 'movie_data_detailed_with_ols.xlsx'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>budget</th>\n",
       "      <th>domgross</th>\n",
       "      <th>title</th>\n",
       "      <th>Response_Json</th>\n",
       "      <th>Year</th>\n",
       "      <th>imdbRating</th>\n",
       "      <th>Metascore</th>\n",
       "      <th>imdbVotes</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13000000</td>\n",
       "      <td>25682380</td>\n",
       "      <td>21 &amp;amp; Over</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>6.8</td>\n",
       "      <td>48</td>\n",
       "      <td>206513</td>\n",
       "      <td>4.912759e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45658735</td>\n",
       "      <td>13414714</td>\n",
       "      <td>Dredd 3D</td>\n",
       "      <td>0</td>\n",
       "      <td>2012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.267265e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20000000</td>\n",
       "      <td>53107035</td>\n",
       "      <td>12 Years a Slave</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>8.1</td>\n",
       "      <td>96</td>\n",
       "      <td>537525</td>\n",
       "      <td>1.626624e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61000000</td>\n",
       "      <td>75612460</td>\n",
       "      <td>2 Guns</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>6.7</td>\n",
       "      <td>55</td>\n",
       "      <td>173726</td>\n",
       "      <td>7.723381e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40000000</td>\n",
       "      <td>95020213</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>7.5</td>\n",
       "      <td>62</td>\n",
       "      <td>74170</td>\n",
       "      <td>4.151958e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     budget  domgross             title  Response_Json  Year  imdbRating  \\\n",
       "0  13000000  25682380     21 &amp; Over              0  2008         6.8   \n",
       "1  45658735  13414714          Dredd 3D              0  2012         0.0   \n",
       "2  20000000  53107035  12 Years a Slave              0  2013         8.1   \n",
       "3  61000000  75612460            2 Guns              0  2013         6.7   \n",
       "4  40000000  95020213                42              0  2013         7.5   \n",
       "\n",
       "   Metascore  imdbVotes         Model  \n",
       "0         48     206513  4.912759e+07  \n",
       "1          0          0  2.267265e+05  \n",
       "2         96     537525  1.626624e+08  \n",
       "3         55     173726  7.723381e+07  \n",
       "4         62      74170  4.151958e+07  "
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('movie_data_detailed_with_ols.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scatter Plot <a id=\"scatter\"></a>  \n",
    "Create a Scatter Plot of the budget and  Domestic Gross (domgross) along with the model column's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1a178f70b8>"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(df.budget, df.domgross, color='blue')\n",
    "plt.scatter(df.budget, df['Model'], color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we get far better results then a simple straight line! Let's start to further investigate how this happens under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent works by finding the steepest slope downhill and taking a step in that direction.  \n",
    "From there, you then iterate and take another step in the steepest direction from that point.  \n",
    "This continues on until you converge on a minimum solution.\n",
    "\n",
    "To write our gradient descent algorithm, we'll need a few things:\n",
    "- An array of [coefficient] weights for the polynomial model. \n",
    "- An error function to evaluate the current iteration's model.\n",
    "- A specified step size coefficient\n",
    "- A precision parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  Create a function predict(X,w) that takes in a matrix of data and a vector of coefficient weights and returns a vector of predicted values.\n",
    "\n",
    "$x_1\\bullet w_1 + x_2\\bullet w_2 + x_3\\bullet w_3 + ... = y_1+y_2+y_3+...$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b=True):\n",
    "    \"\"\"Takes in a matrix X of features, and w,\n",
    "    a vector of coefficients for how much each feature is weighed in the current model.\n",
    "    w[-1] is the constant additive value by default. To remove this feature specify b=False.\"\"\"\n",
    "    if b:\n",
    "        y_hat = np.dot(X, w[:-1])+w[-1] #Multiply coefficient weights by data matrix and add constants\n",
    "        return y_hat\n",
    "    else:\n",
    "        y_hat = np.dot(X, w) #No constant term. Simply mulitply coefficient weights and data.\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Write an error function to calculate the residual sum of squares for a given model.  \n",
    "Your function should take in 3 inputs:\n",
    " * a list of x values\n",
    " * a list of y values (corresponding to the x values passed)\n",
    " * a list of $\\hat{y}$ values produced by the model (corresponding to the x values passed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4298086041804122e+17"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rss(y, y_hat):\n",
    "    return sum((y_hat - y)**2)\n",
    "\n",
    "rss(df['domgross'], df['Model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Putting it all together: Gradient Descent!\n",
    "Initialize a vector of weights for your model. Then calculate the error for this model using the RSS. From there, calculate the gradient at this point. Use that along with your step size coefficient to update your vector of weights. Continue this process until the model converges to an arbitrary precison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 \n",
      "Current weights:\n",
      "[0.572229253, -60161.0423, -1922232.33, 404048.376, 147.980683, 100817606.19154045] \n",
      "RSS Produced: 1.7788171997731178e+17\n",
      "\n",
      "\n",
      "\n",
      "Iteration 500 \n",
      "Current weights:\n",
      "[ 2.92058148e-03 -3.07053904e+02 -9.81081640e+03  2.06220880e+03\n",
      "  7.55273590e-01  5.14559561e+05] \n",
      "RSS Produced: 4.73779092170804e+17\n",
      "\n",
      "\n",
      "\n",
      "Iteration 1000 \n",
      "Current weights:\n",
      "[ 1.49062568e-05 -1.56716201e+00 -5.00730932e+01  1.05252376e+01\n",
      "  3.85481526e-03  2.62624310e+03] \n",
      "RSS Produced: 4.7610440653033754e+17\n",
      "\n",
      "\n",
      "\n",
      "Iteration 1500 \n",
      "Current weights:\n",
      "[ 8.82524419e-08 -9.27837727e-03 -2.96457576e-01  6.23146329e-02\n",
      "  2.28224205e-05  1.55486632e+01] \n",
      "RSS Produced: 4.7611628609710496e+17\n",
      "\n",
      "\n",
      "\n",
      "Iteration 2000 \n",
      "Current weights:\n",
      "[ 6.36662017e-10 -6.69351494e-05 -2.13867485e-03  4.49544047e-04\n",
      "  1.64643243e-07  1.12169625e-01] \n",
      "RSS Produced: 4.761163563391545e+17\n",
      "\n",
      "\n",
      "\n",
      "Iteration 2500 \n",
      "Current weights:\n",
      "[ 4.49948922e-12 -4.73051596e-07 -1.51146828e-05  3.17706811e-06\n",
      "  1.16358520e-09  7.92737753e-04] \n",
      "RSS Produced: 4.7611635684596275e+17\n",
      "\n",
      "\n",
      "\n",
      "Gradient descent converged. Local minimum identified at:\n",
      "Iteration 2707 \n",
      "Current weights:\n",
      "[ 5.87455938e-13 -6.17618923e-08 -1.97338180e-06  4.14799866e-07\n",
      "  1.51918362e-10  1.03500304e-04] \n",
      "RSS Produced: 4.761163568490989e+17\n"
     ]
    }
   ],
   "source": [
    "# w = np.array([X[X.columns[i]].mean() for i in range(X.shape[1])]) #Initialize all weights to column mean\n",
    "# w = [ 6.72229253e-01, -8.01610423e+04, -1.42223233e+06,  6.04048376e+05,\n",
    "#         2.47980683e+02, 130817606.19154045]\n",
    "w = [ 5.72229253e-01, -6.01610423e+04, -1.92223233e+06,  4.04048376e+05,\n",
    "        1.47980683e+02, 100817606.19154045]\n",
    "alpha = .01 # step size multiplier\n",
    "precision = 0.000001\n",
    "previous_step_size = 1 \n",
    "max_iters = 10000 # maximum number of iterations\n",
    "iteration = 0 #iteration counter\n",
    "\n",
    "while (previous_step_size > precision) & (iteration < max_iters):\n",
    "    if iteration%500==0:\n",
    "        print('Iteration {} \\nCurrent weights:\\n{} \\nRSS Produced: {}'.format(iteration, w, rss(y, predict(X, w))))\n",
    "        print('\\n\\n')\n",
    "    #Calculate Nearby Points\n",
    "    sample_steps = np.array(w)/1000.0 #Take mean of feature weights and divide by 100. /\n",
    "                                            #Use this to create surrounding sample points.\n",
    "    #Calculate the Gradient\n",
    "    #Look at weights surrounding our current position.\n",
    "    weights_sample_space = np.array([w+(i*sample_steps) for i in range(-50,51)])\n",
    "    \n",
    "    #Calculate the RSS error for this surrounding weights-space.\n",
    "    y_hats = np.array([predict(X, wi) for wi in weights_sample_space])\n",
    "    rss_weights_sample_space = np.array([rss(y, y_hat) for y_hat in y_hats])\n",
    "    \n",
    "#     weights_and_y_hats = np.concatenate((weights_sample_space,  np.array([rss_weights_sample_space]).T), axis=1)\n",
    "    gradients = np.gradient(rss_weights_sample_space)\n",
    "    steepest_gradient_idx = max(list(enumerate(gradients)), key=lambda x: x[1])[0]\n",
    "    \n",
    "\n",
    "    #Move opposite the gradient by some step size\n",
    "    prev_w = w #Save for calculating how much we moved\n",
    "    w = w - alpha*weights_sample_space[steepest_gradient_idx]\n",
    "    \n",
    "    previous_step_size = np.sqrt(sum([wi**2 for wi in w-prev_w]))\n",
    "\n",
    "    iteration += 1\n",
    "    \n",
    "\n",
    "print(\"Gradient descent converged. Local minimum identified at:\")\n",
    "print('Iteration {} \\nCurrent weights:\\n{} \\nRSS Produced: {}'.format(iteration, w, rss(y, predict(X, w))))\n",
    "#The output for the above will be: ('The local minimum occurs at', 2.2499646074278457)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 \n",
      "Current weights:\n",
      "[0.572229253, -60161.0423, -1922232.33, 404048.376, 147.980683, 100817606.19154045] \n",
      "RSS Produced: 2.638789465246342e+17\n",
      "\n",
      "\n",
      "\n",
      "Iteration 500 \n",
      "Current weights:\n",
      "[ 2.92058148e-03 -3.07053904e+02 -9.81081640e+03  2.06220880e+03\n",
      "  7.55273590e-01  5.14559561e+05] \n",
      "RSS Produced: 4.735240965755706e+17\n",
      "\n",
      "\n",
      "\n",
      "Iteration 1000 \n",
      "Current weights:\n",
      "[ 1.49062568e-05 -1.56716201e+00 -5.00730932e+01  1.05252376e+01\n",
      "  3.85481526e-03  2.62624310e+03] \n",
      "RSS Produced: 4.761030869908907e+17\n",
      "\n",
      "\n",
      "\n",
      "Iteration 1500 \n",
      "Current weights:\n",
      "[ 8.68147323e-08 -9.12722437e-03 -2.91628022e-01  6.12994729e-02\n",
      "  2.24506233e-05  1.52953619e+01] \n",
      "RSS Produced: 4.761162795641286e+17\n",
      "\n",
      "\n",
      "\n",
      "Iteration 2000 \n",
      "Current weights:\n",
      "[ 6.32034948e-10 -6.64486848e-05 -2.12313160e-03  4.46276895e-04\n",
      "  1.63446665e-07  1.11354409e-01] \n",
      "RSS Produced: 4.7611635628691085e+17\n",
      "\n",
      "\n",
      "\n",
      "Iteration 2500 \n",
      "Current weights:\n",
      "[ 4.54406201e-12 -4.77737734e-07 -1.52644117e-05  3.20854075e-06\n",
      "  1.17511189e-09  8.00590763e-04] \n",
      "RSS Produced: 4.7611635684552474e+17\n",
      "\n",
      "\n",
      "\n",
      "Gradient descent converged. Local minimum identified at:\n",
      "Iteration 2709 \n",
      "Current weights:\n",
      "[ 5.77453452e-13 -6.07102859e-08 -1.93978146e-06  4.07737158e-07\n",
      "  1.49331681e-10  1.01738026e-04] \n",
      "RSS Produced: 4.761163568490559e+17\n"
     ]
    }
   ],
   "source": [
    "# w = np.array([X[X.columns[i]].mean() for i in range(X.shape[1])]) #Initialize all weights to column mean\n",
    "# w = [ 6.72229253e-01, -8.01610423e+04, -1.42223233e+06,  6.04048376e+05,\n",
    "#         2.47980683e+02, 130817606.19154045]\n",
    "w = [ 5.72229253e-01, -6.01610423e+04, -1.92223233e+06,  4.04048376e+05,\n",
    "        1.47980683e+02, 100817606.19154045]\n",
    "alpha = .01 # step size multiplier\n",
    "precision = 0.000001\n",
    "previous_step_size = 1 \n",
    "max_iters = 10000 # maximum number of iterations\n",
    "iteration = 0 #iteration counter\n",
    "\n",
    "while (previous_step_size > precision) & (iteration < max_iters):\n",
    "    if iteration%500==0:\n",
    "        print('Iteration {} \\nCurrent weights:\\n{} \\nRSS Produced: {}'.format(iteration, w, rss(y, predict(X, w))))\n",
    "        print('\\n\\n')\n",
    "    #Calculate Nearby Points\n",
    "    sample_steps = np.array(w)/1000.0 #Take mean of feature weights and divide by 100. /\n",
    "                                            #Use this to create surrounding sample points.\n",
    "    #Calculate the Gradient\n",
    "    #Look at weights surrounding our current position.\n",
    "    weights_sample_space = np.array([w+(i*sample_steps) for i in range(-50,51)])\n",
    "    \n",
    "    #Calculate the RSS error for this surrounding weights-space.\n",
    "    y_hats = np.array([predict(X, wi) for wi in weights_sample_space])\n",
    "    rss_weights_sample_space = np.array([rss(y, y_hat) for y_hat in y_hats])\n",
    "    \n",
    "#     weights_and_y_hats = np.concatenate((weights_sample_space,  np.array([rss_weights_sample_space]).T), axis=1)\n",
    "    gradients = np.gradient(rss_weights_sample_space)\n",
    "    steepest_gradient_idx = max(list(enumerate(gradients)), key=lambda x: x[1])[0]\n",
    "    \n",
    "\n",
    "    #Move opposite the gradient by some step size\n",
    "    prev_w = w #Save for calculating how much we moved\n",
    "    w = w - alpha*weights_sample_space[steepest_gradient_idx]\n",
    "    \n",
    "    previous_step_size = np.sqrt(sum([wi**2 for wi in w-prev_w]))\n",
    "\n",
    "    iteration += 1\n",
    "    \n",
    "\n",
    "print(\"Gradient descent converged. Local minimum identified at:\")\n",
    "print('Iteration {} \\nCurrent weights:\\n{} \\nRSS Produced: {}'.format(iteration, w, rss(y, predict(X, w))))\n",
    "#The output for the above will be: ('The local minimum occurs at', 2.2499646074278457)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "Found New Minimum: 3.640817242349461e+30!\n",
      "0 (6,) 3.640817242349461e+30\n",
      "3.640817242349461e+30\n",
      "1 (6,) 3.644877226030093e+30\n",
      "3.644877226030093e+30\n",
      "2 (6,) 3.652997193391717e+30\n",
      "3.652997193391717e+30\n",
      "3 (6,) 3.6611171607537736e+30\n",
      "3.6611171607537736e+30\n",
      "4 (6,) 3.669237128115614e+30\n",
      "3.669237128115614e+30\n",
      "5 (6,) 3.6773570954773103e+30\n",
      "3.6773570954773103e+30\n"
     ]
    }
   ],
   "source": [
    "minimum = 5*10**100\n",
    "# print(minimum)\n",
    "for n, weights_y_hat in enumerate(gradient):\n",
    "    dy = weights_y_hat[-1]\n",
    "    if dy < minimum:\n",
    "        print('Found New Minimum: {}!'.format(dy))\n",
    "        minimum = min(minimum, dy)\n",
    "    if n<6:\n",
    "        print(n, weights_y_hat.shape, weights_y_hat[-1])\n",
    "        print(dy)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
